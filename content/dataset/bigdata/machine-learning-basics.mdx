# 빅데이터분석기사 - 머신러닝 기초

## 개요
빅데이터분석기사 실기 시험을 위한 머신러닝 기초 개념과 Python 실습을 다룹니다.

## 1. 머신러닝 개요

### 머신러닝의 유형

#### 지도학습 (Supervised Learning)
- **회귀(Regression)**: 연속적인 수치 예측
- **분류(Classification)**: 범주형 데이터 예측

#### 비지도학습 (Unsupervised Learning)
- **군집화(Clustering)**: 유사한 데이터 그룹핑
- **차원축소(Dimensionality Reduction)**: 데이터 차원 감소

#### 강화학습 (Reinforcement Learning)
- 환경과의 상호작용을 통한 학습

## 2. 지도학습 - 선형회귀

### 단순 선형회귀

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns

# 샘플 데이터 생성
np.random.seed(42)
X = np.random.rand(100, 1) * 10
y = 2.5 * X.flatten() + np.random.randn(100) * 2 + 10

# 데이터프레임 생성
df = pd.DataFrame({'X': X.flatten(), 'y': y})

# 훈련/테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 선형회귀 모델 학습
model = LinearRegression()
model.fit(X_train, y_train)

# 예측
y_pred = model.predict(X_test)

# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"평균제곱오차(MSE): {mse:.3f}")
print(f"결정계수(R²): {r2:.3f}")
print(f"회귀계수: {model.coef_[0]:.3f}")
print(f"절편: {model.intercept_:.3f}")
```

```
평균제곱오차(MSE): 4.123
결정계수(R²): 0.876
회귀계수: 2.487
절편: 10.124
```

### 시각화

```python
plt.figure(figsize=(12, 5))

# 원본 데이터와 회귀선
plt.subplot(1, 2, 1)
plt.scatter(X, y, alpha=0.6, color='blue', label='실제 데이터')
plt.plot(X, model.predict(X), color='red', linewidth=2, label='회귀선')
plt.xlabel('X')
plt.ylabel('y')
plt.title('선형회귀 결과')
plt.legend()
plt.grid(True, alpha=0.3)

# 잔차 플롯
plt.subplot(1, 2, 2)
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, alpha=0.6)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel('예측값')
plt.ylabel('잔차')
plt.title('잔차 플롯')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 3. 지도학습 - 분류

### 로지스틱 회귀

```python
from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# 분류 데이터 생성
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0,
                          n_informative=2, n_clusters_per_class=1, random_state=42)

# 훈련/테스트 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 로지스틱 회귀 모델
log_model = LogisticRegression(random_state=42)
log_model.fit(X_train, y_train)

# 예측
y_pred = log_model.predict(X_test)
y_pred_proba = log_model.predict_proba(X_test)

# 성능 평가
print("분류 성능 보고서:")
print(classification_report(y_test, y_pred))
```

```
분류 성능 보고서:
              precision    recall  f1-score   support

           0       0.89      0.92      0.91       103
           1       0.91      0.88      0.89        97

    accuracy                           0.90       200
   macro avg       0.90      0.90      0.90       200
weighted avg       0.90      0.90      0.90       200
```

### 혼동행렬 시각화

```python
# 혼동행렬
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(12, 5))

# 데이터 분포
plt.subplot(1, 2, 1)
colors = ['red' if label == 0 else 'blue' for label in y]
plt.scatter(X[:, 0], X[:, 1], c=colors, alpha=0.6)
plt.title('분류 데이터 분포')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

# 혼동행렬
plt.subplot(1, 2, 2)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Class 0', 'Class 1'],
            yticklabels=['Class 0', 'Class 1'])
plt.title('혼동행렬')
plt.ylabel('실제 클래스')
plt.xlabel('예측 클래스')

plt.tight_layout()
plt.show()
```

## 4. 비지도학습 - 군집화

### K-Means 클러스터링

```python
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 군집 데이터 생성
X_cluster, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=42)

# K-Means 클러스터링
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_cluster)

# 최적 클러스터 수 찾기 (엘보우 방법)
inertias = []
k_range = range(1, 11)

for k in k_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans_temp.fit(X_cluster)
    inertias.append(kmeans_temp.inertia_)

plt.figure(figsize=(12, 5))

# 클러스터링 결과
plt.subplot(1, 2, 1)
colors = ['red', 'blue', 'green', 'orange', 'purple']
for i in range(4):
    cluster_points = X_cluster[cluster_labels == i]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1],
               c=colors[i], label=f'Cluster {i}', alpha=0.7)

# 클러스터 중심점
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=200, linewidths=3)
plt.title('K-Means 클러스터링 결과')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True, alpha=0.3)

# 엘보우 방법
plt.subplot(1, 2, 2)
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('클러스터 수 (k)')
plt.ylabel('관성(Inertia)')
plt.title('엘보우 방법 - 최적 클러스터 수')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

## 5. 모델 평가 및 선택

### 교차검증

```python
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

# 교차검증을 위한 분류 데이터
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5,
                          n_redundant=5, random_state=42)

# 랜덤포레스트 모델
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# 5-Fold 교차검증
cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='accuracy')

print(f"교차검증 점수: {cv_scores}")
print(f"평균 정확도: {cv_scores.mean():.3f} (±{cv_scores.std() * 2:.3f})")

# 계층적 K-Fold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
stratified_scores = cross_val_score(rf_model, X, y, cv=skf, scoring='accuracy')

print(f"계층적 교차검증 점수: {stratified_scores}")
print(f"계층적 평균 정확도: {stratified_scores.mean():.3f} (±{stratified_scores.std() * 2:.3f})")
```

```
교차검증 점수: [0.89  0.915 0.89  0.905 0.885]
평균 정확도: 0.897 (±0.024)
계층적 교차검증 점수: [0.895 0.91  0.885 0.9   0.89 ]
계층적 평균 정확도: 0.896 (±0.020)
```

## 6. 특성 중요도 분석

```python
# 특성 중요도 시각화
feature_names = [f'Feature {i+1}' for i in range(10)]
rf_model.fit(X, y)
importances = rf_model.feature_importances_

plt.figure(figsize=(10, 6))
indices = np.argsort(importances)[::-1]

plt.bar(range(len(importances)), importances[indices])
plt.xlabel('특성')
plt.ylabel('중요도')
plt.title('랜덤포레스트 특성 중요도')
plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## 연습문제

:::warning[실기 문제 1]
다음 데이터에 대해 선형회귀 모델을 구축하고 MSE와 R²를 계산하시오.
- 독립변수: 광고비 (단위: 만원)
- 종속변수: 매출액 (단위: 만원)
:::

<CollapsibleCodeCell code={`print(f"R²: {r2:.3f}")`} />

:::warning[실기 문제 2]
주어진 데이터에 대해 K-Means 클러스터링을 수행하고 최적의 클러스터 수를 엘보우 방법으로 결정하시오.
:::

## 참고자료

- 빅데이터분석기사 실기 시험 안내
- scikit-learn 공식 문서
- 『파이썬 머신러닝 완벽 가이드』, 권철민